 I'm a final year PhD student at CMU advised by [Prof. Zico Kolter](http://zicokolter.com/) and [Prof. Zac Manchester](http://roboticexplorationlab.org/). Developing RL and iterated inference based reasoning algorithms for deep networks across applications spanning multi-agent LLMs, robotic control, 3D vision, generative models and differentiable solvers. Published at NeurIPS, CVPR, CoRL, ICRA, AAMAS. Blending deep algorithmic understanding with pragmatic engineering to deliver robust, reliable systems. My publications and descriptions of some selected projects are available below and on [my Google Scholar page](https://scholar.google.com/citations?user=do8COWIAAAAJ&hl=en). 

I also carely deeply about ensuring that we build AI that benefits humanity and ensuring our social infrastructure can adapt to it. I've written various blogs discussing my thoughts on various topics, from 'the economic and governance infrastructure for the agentic web' to 'where the next scaling laws for LLMs will come from'. Follow my [Blog](https://swaminathangurumurthy805874.substack.com/profile/posts).


## <i class="fa fa-chevron-right"></i> Education

<table class="table table-hover">
  <tr>
    <td class="col-md-3">August 2020 - Present</td>
    <td>
        <strong>Ph.D. in Robotics Research</strong>
          (0.00/0.00)
        <br>
      Carnegie Mellon University
    </td>
  </tr>
  <tr>
    <td class="col-md-3">Aug 2017 - Dec 2019</td>
    <td>
        <strong>M.S. in Robotics Research</strong>
          (4.09/4.33)
        <br>
      Carnegie Mellon University
    </td>
  </tr>
  <tr>
    <td class="col-md-3">July 2013 - May 2017</td>
    <td>
        <strong>B.S. in Electrical Engineering</strong>
          (8.99/10.00)
        <br>
      IIT-BHU Varanasi
    </td>
  </tr>
</table>


<!-- ## <i class="fa fa-chevron-right"></i> Experience
<table class="table table-hover">
<tr>
  <td class='col-md-3'>May 2019 - Present</td>
  <td><strong>Facebook AI</strong>, Research Scientist</td>
</tr>
<tr>
</tr>
<tr>
  <td class='col-md-3'>June 2018 - Sept 2018</td>
  <td><strong>Intel Labs</strong>, Research Intern</td>
</tr>
<tr>
</tr>
<tr>
  <td class='col-md-3'>May 2017 - Oct 2017</td>
  <td><strong>Google DeepMind</strong>, Research Intern</td>
</tr>
<tr>
</tr>
<tr>
  <td class='col-md-3'>May 2014 - Aug 2014</td>
  <td><strong>Adobe Research</strong>, Data Scientist Intern</td>
</tr>
<tr>
</tr>
<tr>
  <td class='col-md-3'>Dec 2013 - Jan 2014</td>
  <td><strong>Snowplow Analytics</strong>, Software Engineer Intern</td>
</tr>
<tr>
</tr>
<tr>
  <td class='col-md-3'>May 2013 - Aug 2013</td>
  <td><strong>Qualcomm</strong>, Software Engineer Intern</td>
</tr>
<tr>
</tr>
<tr>
  <td class='col-md-3'>May 2012 - Aug 2012</td>
  <td><strong>Phoenix Integration</strong>, Software Engineer Intern</td>
</tr>
<tr>
</tr>
<tr>
  <td class='col-md-3'>Jan 2011 - Aug 2011</td>
  <td><strong>Sunapsys</strong>, Network Administrator Intern</td>
</tr>
<tr>
</tr>
</table> -->

## <i class="fa fa-chevron-right"></i> Blog Posts and Research Vision <i class="fa fa-code-fork" aria-hidden="true"></i>

<table class="table table-hover">

<tr>
<td class="col-md-3"><a href='https://swaminathangurumurthy805874.substack.com/p/economic-trust-and-governance-infrastructure' target='_blank'><img src="images/publications/agentic_web.png"/></a> </td>
<td>
    <strong>Scaffolding of Trust : Reimagining Economics and Governance for the Agentic Web</strong><br>
    <strong>Swaminathan Gurumurthy</strong>, J Zico Kolter, Zachary Manchester<br>
    To be submitted to ICLR 2026 Blogs<br>
    
    [1] 
[<a href='https://swaminathangurumurthy805874.substack.com/p/economic-trust-and-governance-infrastructure' 
    target='_blank'>pdf</a>] <br>
    
<!-- <div id="abs_amos2020differentiable" style="text-align: justify; display: none" markdown="1"> -->
We discuss the economic and governance tools and frameworks required for the emerging AI-driven internet. We propose various tools and mechanisms to this end such as multi-sided reputation markets, agentic marketplaces, attribution-led pricing, auction-first markets, and a framework for polycentric governance and the tools required.
<!-- </div> -->

</td>
</tr>


<tr>
<td class="col-md-3"><a href='https://swaminathangurumurthy805874.substack.com/p/meta-learning-the-next-scaling-law' target='_blank'><img src="images/publications/meta-learning.png"/></a> </td>
<td>
    <strong>Meta-Learning and Learning to Search : The Next Scaling Laws </strong><br>
    <strong>Swaminathan Gurumurthy</strong>, J Zico Kolter, Zachary Manchester<br>
    To be submitted to ICLR 2026 Blogs<br>
    
    [1] 
[<a href='https://swaminathangurumurthy805874.substack.com/p/meta-learning-the-next-scaling-law' 
    target='_blank'>pdf</a>] <br>
    
<!-- <div id="abs_amos2020differentiable" style="text-align: justify; display: none" markdown="1"> -->
While data scaling and test-time compute built today's AI systems, the next generation will be defined by two new scaling laws hiding in plain sight: parallel search and meta-learning. This article explores how training models to perform native parallel reasoning and to reliably fine-tune themselves on specialized tasks could transform ad-hoc techniques into capital-absorbing scaling laws with predictable performance gains and qualititively new capabilities—from static knowledge artifacts to dynamic systems capable of autonomous exploration and specialized adaptation. 
<!-- </div> -->

</td>
</tr>

<tr>
<td class="col-md-3"><a href='https://swaminathangurumurthy805874.substack.com/p/managing-trust-in-llm-agent-platforms' target='_blank'><img src="images/publications/trust_markets.png"/></a> </td>
<td>
    <strong>Trust Markets : Market based Reputation System for Multi-Agent LLM Platforms</strong><br>
    <strong>Swaminathan Gurumurthy</strong>, J Zico Kolter, Zachary Manchester<br>
    In Progress (ICML 2026)<br>
    
    [1] 
[<a href='https://swaminathangurumurthy805874.substack.com/p/managing-trust-in-llm-agent-platforms' 
    target='_blank'>pdf</a>] <br>
    
<!-- <div id="abs_amos2020differentiable" style="text-align: justify; display: none" markdown="1"> -->
We co-design and implement a market based reputation system and the corresponding scaffolding for LLM agents. The reputation system serves as a dynamic alignment system for LLM agent platforms where different agent providers compute for better reputation and trust. We further investigate various elements of the LLM scaffolding that are important including, computing iterated equilibria to reach faster and stable equilibria, using optimization based investing as tools for the LLMs and bayesian averaging of the LLM preferences over time to ensure the stability of the preferences and the equilibrium. 
<!-- </div> -->

</td>
</tr>

</table>

## <i class="fa fa-chevron-right"></i> Publications and Selected Projects <i class="fa fa-code-fork" aria-hidden="true"></i>

<a href="https://scholar.google.com/citations?user=do8COWIAAAAJ&hl=en&oi=sra" class="btn btn-primary" style="padding: 0.3em;">
  <i class="ai ai-google-scholar"></i> Google Scholar
</a>

<table class="table table-hover">


<tr>
<td class="col-md-3"><a href='https://swaminathangurumurthy805874.substack.com/p/managing-trust-in-llm-agent-platforms' target='_blank'><img src="images/publications/trust_markets.png"/></a> </td>
<td>
    <strong>Trust Markets : Market based Reputation System for Multi-Agent LLM Platforms</strong><br>
    <strong>Swaminathan Gurumurthy</strong>, J Zico Kolter, Zachary Manchester<br>
    In Progress (ICML 2026)<br>
    
    [1] 
[<a href='https://swaminathangurumurthy805874.substack.com/p/managing-trust-in-llm-agent-platforms' 
    target='_blank'>pdf</a>] [<a href='https://github.com/swami1995/LLM_MD' target='_blank'>code</a>] <br>
    
<!-- <div id="abs_amos2020differentiable" style="text-align: justify; display: none" markdown="1"> -->
We co-design and implement a market based reputation system and the corresponding scaffolding for LLM agents. The reputation system serves as a dynamic alignment system for LLM agent platforms where different agent providers compute for better reputation and trust. We further investigate various elements of the LLM scaffolding that are important including, computing iterated equilibria to reach faster and stable equilibria, using optimization based investing as tools for the LLMs and bayesian averaging of the LLM preferences over time to ensure the stability of the preferences and the equilibrium. 
<!-- </div> -->

</td>
</tr>


<tr>
<td class="col-md-3"><a href='https://openreview.net/forum?id=zQXurgHUVX' target='_blank'><img src="images/publications/deq-mpc.png"/></a> </td>
<td>
    <strong>DEQ-MPC : Deep Equilibrium Model Predictive Control</strong><br>
    <strong>Swaminathan Gurumurthy</strong>, Khai Nguyen, Arun L Bishop, J Zico Kolter, Zachary Manchester<br>
    CoRL 2025<br>
    
    [1] 
[<a href='https://openreview.net/forum?id=zQXurgHUVX' 
    target='_blank'>pdf</a>] [<a href='https://github.com/swami1995/DEQ-MPC' target='_blank'>code</a>] <br>
    
<!-- <div id="abs_amos2020differentiable" style="text-align: justify; display: none" markdown="1"> -->
We propose a novel approach that co-develops the solver and architecture unifying the optimization solver and network inference problems. Specifically, we formulate this as a \textit{joint fixed-point problem} over the coupled network outputs and necessary conditions of the optimization problem. Through extensive ablations in various robotic control tasks, we demonstrate that our approach results in richer representations and more stable training, while naturally accommodating warm starting, a key requirement for optimal control problems.
<!-- </div> -->

</td>
</tr>


<tr>
<td class="col-md-3"><a href='https://arxiv.org/abs/2406.07785' target='_blank'><img src="images/publications/v2v.png"/></a> </td>
<td>
    <strong>From Variance to Veracity: Unbundling and Mitigating Gradient Variance in Differentiable Bundle Adjustment Layers</strong><br>
    <strong>Swaminathan Gurumurthy</strong>, Karnik Ram, Bingqing Chen, Zachary Manchester, Zico Kolter<br>
    CVPR 2024<br>
    
    [1] 
[<a href='https://arxiv.org/abs/2406.07785' 
    target='_blank'>pdf</a>] [<a href='https://github.com/swami1995/V2V' target='_blank'>code</a>] <br>
    
<!-- <div id="abs_amos2020differentiable" style="text-align: justify; display: none" markdown="1"> -->
We identify/show three causes for gradient instability and gradient variance when using differentiable bundle adjustment layers within iterative refinement models : (1) flow loss interference, (2) linearization errors in the bundle adjustment (BA) layer, and (3) dependence of weight gradients on the BA residual. We then propose a simple, yet effective solution to reduce the gradient variance by using the weights predicted by the network in the inner optimization loop to weight the correspondence objective in the training problem. 
<!-- </div> -->

</td>
</tr>

<tr>
<td class="col-md-3"><a href='https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610864' target='_blank'><img src="images/publications/vinsat.png"/></a> </td>
<td>
    <strong>VINSat: Solving the Lost-in-Space Problem with Visual-Inertial Navigation</strong><br>
    Kyle McCleary, <strong>Swaminathan Gurumurthy</strong>, Paulo RM Fisch, Saral Tayal, Zachary Manchester, Brandon Lucia<br>
    ICRA 2024<br>
    
    [1] 
[<a href='https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10610864' 
    target='_blank'>pdf</a>] [<a href='https://github.com/CMUAbstract/VINSat' target='_blank'>code</a>] <br>
    
<!-- <div id="abs_amos2020differentiable" style="text-align: justify; display: none" markdown="1"> -->
This paper introduces a solution to this “lost-in-space” problem that we call Visual Inertial Navigation for Satellites (VINSat). VINSat performs OD using data from an inertial measurement unit (IMU) and a low-cost RGB camera. We train an object detector to detect landmarks on the earth's surface and then solve a batch optimization problem for state estimation of the satellite given the detected landmarks. 
<!-- </div> -->

</td>
</tr>

<tr>
<td class="col-md-3"><a href='https://arxiv.org/abs/2311.18056' target='_blank'><img src="images/publications/reluqp.png"/></a> </td>
<td>
    <strong>ReLU-QP: A GPU-Accelerated Quadratic Programming Solver for Model-Predictive Control</strong><br>
    Arun L. Bishop*, John Z. Zhang*, <strong>Swaminathan Gurumurthy</strong>, Kevin Tracy, Zachary Manchester<br>
    ICRA 2024<br>
    
    [1] 
[<a href='https://arxiv.org/abs/2311.18056' 
    target='_blank'>pdf</a>] [<a href='https://github.com/RoboticExplorationLab/ReLUQP-py' target='_blank'>code</a>] <br>
    
<!-- <div id="abs_amos2020differentiable" style="text-align: justify; display: none" markdown="1"> -->
We present ReLU-QP, a GPU-accelerated solver for quadratic programs (QPs) that is capable of solving high-dimensional control problems at real-time rates. ReLU-QP is derived by exactly reformulating the Alternating Direction Method of Multipliers (ADMM) algorithm for solving QPs as a deep, weight-tied neural network with rectified linear unit (ReLU) activations. This reformulation enables the deployment of ReLU-QP on GPUs using standard machine-learning toolboxes.
<!-- </div> -->

</td>
</tr>

<tr>
<td class="col-md-3"><a href='https://arxiv.org/abs/2304.14389' target='_blank'><img src="images/publications/slomo.png"/></a> </td>
<td>
    <strong>SLoMo: A General System for Legged Robot Motion Imitation from Casual Videos</strong><br>
    John Z. Zhang, Shuo Yang, Gengshan Yang, Arun L. Bishop, <strong>Swaminathan Gurumurthy</strong>, Deva Ramanan, Zachary Manchester<br>
    RA-L 2023<br>
    
    [1] 
[<a href='https://arxiv.org/pdf/2304.14389' 
    target='_blank'>pdf</a>] [<a href='https://github.com/johnzhang3/SLoMo' target='_blank'>code</a>] <br>
    
<!-- <div id="abs_amos2020differentiable" style="text-align: justify; display: none" markdown="1"> -->
We present SLoMo: a first-of-its-kind framework for transferring skilled motions from casually captured "in the wild" video footage of humans and animals to legged robots. SLoMo works in three stages: 1. synthesize a physically plausible reconstructed key-point trajectory from monocular videos; 2. optimize a dynamically feasible reference trajectory for the robot offline that includes body and foot motion, as well as contact sequences that closely tracks the key points; 3. track the reference trajectory online using a general-purpose model-predictive controller on robot hardware
<!-- </div> -->

</td>
</tr>

<tr>
<td class="col-md-3"><a href='https://proceedings.mlr.press/v211/gurumurthy23b.html' target='_blank'><img src="images/publications/deepilc.png"/></a> </td>
<td>
    <strong>Deep Off-Policy Iterative Learning Control</strong><br>
    <strong>Swaminathan Gurumurthy</strong>, Zico Kolter, Zachary Manchester<br>
    L4DC 2023<br>
    
    [1] 
[<a href='https://proceedings.mlr.press/v211/gurumurthy23b/gurumurthy23b.pdf' 
    target='_blank'>pdf</a>] [<a href='https://github.com/RoboticExplorationLab/Deep-ILC' target='_blank'>code</a>] <br>
    
<!-- <div id="abs_amos2020differentiable" style="text-align: justify; display: none" markdown="1"> -->
The paper proposes an update equation for the value-function gradients to speed up actor critic methods in reinforcement learning. This update is inspired by iterative learning control (ILC) approaches that use approximate simulator gradients to speed up optimization. The value-gradient update leads to a significant improvement in sample efficiency and sometimes better performance both when learning from scratch or fine-tuning a pre-trained policy in a new environment.
<!-- </div> -->

</td>
</tr>

<tr>
<td class="col-md-3"><a href='https://proceedings.mlr.press/v211/gurumurthy23a.html' target='_blank'><img src="images/publications/deepilc.png"/></a> </td>
<td>
    <strong>Practical Critic Gradient based Actor Critic for On-Policy Reinforcement Learning</strong><br>
    <strong>Swaminathan Gurumurthy</strong>, Zachary Manchester, Zico Kolter<br>
    L4DC 2023<br>
    
    [1] 
[<a href='https://proceedings.mlr.press/v211/gurumurthy23a/gurumurthy23a.pdf' 
    target='_blank'>pdf</a>] [<a href='https://github.com/RoboticExplorationLab/CGAC' target='_blank'>code</a>] <br>
    
<!-- <div id="abs_amos2020differentiable" style="text-align: justify; display: none" markdown="1"> -->
In this paper, we present a different class of on-policy algorithms based on SARSA, which estimate the policy gradient using the critic-action gradients. We show that they are better suited than existing baselines (like PPO) especially when using highly parallelizable simulators. We observe that the critic gradient based on-policy method (CGAC) consistently achieves higher episode returns. Furthermore, in environments with high dimensional action space, CGAC also trains much faster (in wall-clock time) than the corresponding baselines.
<!-- </div> -->

</td>
</tr>

<tr>
<td class="col-md-3"><a href='https://arxiv.org/abs/2111.13236' target='_blank'><img src="images/publications/jiio.png"/></a> </td>
<td>
    <strong>Joint inference and input optimization in equilibrium networks</strong><br>
    <strong>Swaminathan Gurumurthy</strong>, Shaojie Bai, Zachary Manchester, Zico Kolter<br>
    Neurips 2021<br>
    
    [1] 
[<a href='https://arxiv.org/pdf/2111.13236' 
    target='_blank'>pdf</a>] [<a href='https://github.com/locuslab/JIIO-DEQ' target='_blank'>code</a>] <br>
    
<!-- <div id="abs_amos2020differentiable" style="text-align: justify; display: none" markdown="1"> -->
We propose to use Deep Equilibrium Models to solve inverse problems efficiently by exploiting the iterative nature of the DEQ fixed point process. We simultaneously solve for the DEQ fixed point and optimize over network inputs, all within a single augmented DEQ model that jointly encodes both the original network and the optimization process. Indeed, the procedure is fast enough that it allows us to efficiently train DEQ models for tasks traditionally relying on an inner optimization loop.
<!-- </div> -->

</td>
</tr>

<tr>
<td class="col-md-3"><a href='https://arxiv.org/abs/1911.04024' target='_blank'><img src="images/publications/mame2019.png"/></a> </td>
<td>
    <strong>MAME : Model Agnostic Meta Exploration</strong><br>
    <strong>Swaminathan Gurumurthy</strong>, Sumit Kumar, Katia Sycara<br>
    CoRL 2019<br>
    
    [1] 
[<a href='https://arxiv.org/abs/1911.04024' 
    target='_blank'>pdf</a>] [<a href='https://github.com/swami1995/exp_maml/tree/another_sparse_branch_ppo' target='_blank'>code</a>] <br>
    
<!-- <div id="abs_amos2020differentiable" style="text-align: justify; display: none" markdown="1"> -->
We propose to explicitly model a separate exploration policy for the task distribution in Meta-RL given the requirements on sample efficiency. Having two different policies gives more flexibility during training and makes adaptation to any specific task easier. We show that using self-supervised or supervised learning objectives for adaptation stabilizes the training process and improves performance.
<!-- </div> -->

</td>
</tr>


<tr>
<td class="col-md-3"><a href='https://arxiv.org/abs/1808.04359' target='_blank'><img src="images/publications/visdial2019.png"/></a> </td>
<td>
    <strong>Community Regularization of Visually-Grounded Dialog</strong><br>
    Akshat Agarwal*, <strong>Swaminathan Gurumurthy*</strong>, Vasu Sharma*, Katia Sycara, Michael Lewis<br>
    AAMAS 2019 <strong>[Oral talk]</strong><br>
    
    [2] 
[<a href='https://arxiv.org/abs/1808.04359' 
    target='_blank'>pdf</a>] [<a href='https://github.com/agakshat/visualdialog-pytorch' target='_blank'>code</a>] <br>
    
<!-- <div id="abs_amos2020differentiable" style="text-align: justify; display: none" markdown="1"> -->
We aim to train 2 agents on the visual dialogue dataset where one agent is given access to an image and the other agent is tasked with guessing the contents of the image by establishing a dialogue with the first agent. The two agents are initially trained using supervision followed by Reinforce. In order to combat the resulting drift from natural language when training with Reinforce, we introduce a community regularization scheme of training a population of agents.
<!-- </div> -->

</td>
</tr>


<tr>
<td class="col-md-3"><a href='https://arxiv.org/abs/1807.03407' target='_blank'><img src="images/publications/pcc2019.png"/></a> </td>
<td>
    <strong> 3D Point Cloud Completion using Latent Optimization in GANs </strong><br>
    Shubham Agarwal*, <strong>Swaminathan Gurumurthy*</strong> <br>
    WACV 2019<br>
    
    [3] 
[<a href='https://arxiv.org/abs/1807.03407' target='_blank'>pdf</a>] <br>
    
<!-- <div id="abs_amos2020differentiable" style="text-align: justify; display: none" markdown="1"> -->
We address a fundamental problem with Neural Network based point cloud completion methods which reconstruct the entire structure rather than preserving the points already provided as input. These methods struggle when tested on unseen deformities. We address this problem by introducing a GAN based Latent optimization procedure to perform output constrained optimization using the regions provided in the input.
<!-- </div> -->

</td>
</tr>



<tr>
<td class="col-md-3"><a href='https://arxiv.org/abs/1805.05356' target='_blank'><img src="images/publications/compass2018.png"/></a> </td>
<td>
    <strong>Exploiting Data and Human Knowledge for Predicting Wildlife Poaching</strong><br>
    <strong>Swaminathan Gurumurthy</strong>, Lantao Yu, Chenyan Zhang, Yongchao Jin, Weiping Li, Haidong Zhang, Fei Fang<br>
    COMPASS 2019 <strong>[Oral talk]</strong><br>

    [4] 
[<a href='https://arxiv.org/abs/1805.05356' 
    target='_blank'>pdf</a>] [<a href='https://github.com/swami1995/PAWS-COMPASS' target='_blank'>code</a>] <br>
    
<!-- <div id="abs_amos2020differentiable" style="text-align: justify; display: none" markdown="1"> -->
Using past data of traps/snares found in a wildlife Sanctuary, we predict the regions of high probability of traps/snares to guide the rangers to patrol those regions. We use novel frameworks of incorporating expert domain knowledge for the dynamic sampling of data points in order to tackle the imbalance in data. We further use these regions to produce optimal patrol routes for the rangers. This has now been deployed in a conservation area in China.
<!-- </div> -->

</td>
</tr>


<tr>
<td class="col-md-3"><a href='http://openaccess.thecvf.com/content_cvpr_2017/papers/Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper.pdf' target='_blank'><img src="images/publications/deligan2017.png"/></a> </td>
<td>
    <strong> DeLiGAN: GANs for Diverse and Limited Data </strong><br>
    <strong>Swaminathan Gurumurthy*</strong>, Ravi Kiran S.* and R. Venkatesh Babu <br>
    CVPR 2017<br>

    [5] 
[<a href='http://openaccess.thecvf.com/content_cvpr_2017/papers/Gurumurthy_DeLiGAN__Generative_CVPR_2017_paper.pdf' 
    target='_blank'>pdf</a>] [<a href='https://github.com/val-iisc/deligan' target='_blank'>code</a>] <br>
    
<!-- <div id="abs_amos2020differentiable" style="text-align: justify; display: none" markdown="1"> -->
We try to explore the idea of finding high probability regions in the latent space of GANs by learning a latent space representation using learnable Mixture of Gaussians. This enables the GAN to model a multimodal distribution and stabilizes training as observed visually and by the intra-class variance measured using a modified inception score. Our modification is especially useful when the dataset is very small and diverse.
<!-- </div> -->

</td>
</tr>

<tr>
<td class="col-md-3"><a href='https://drive.google.com/file/d/1l8ngpk-A3E01gTBhAsXT7IzV_bIJ_b12/view?usp=sharing' target='_blank'><img src="images/publications/bbox_attack.png"/></a> </td>
<td>
    <strong> Query Efficient Black Box Attacks in Neural Networks </strong><br>
    <strong>Swaminathan Gurumurthy</strong>, Fei Fang and Martial Hebert <br>

    [6] 
[<a href='https://drive.google.com/file/d/1l8ngpk-A3E01gTBhAsXT7IzV_bIJ_b12/view?usp=sharing' 
    target='_blank'>pdf</a>] <br>
    
<!-- <div id="abs_amos2020differentiable" style="text-align: justify; display: none" markdown="1"> -->
We test various methods to increase the sample efficiency of adversarial black box attacks on Neural nets. In one of the methods, we analyze the transferability of gradients and find that it has two components: Network specific components and Task specific components. The task specific component corresponds to the transferable properties of adversarial examples between architectures. Hence, we attempted to isolate this component and enhance the transfer properties. We then perform multiple queries on the black box network to obtain the architecture specific components using ES.
<!-- </div> -->

</td>
</tr>


<tr>
<td class="col-md-3"><img src="images/publications/slam.png"/></td>
<td>
    <strong> Visual SLAM based SfM for Boreholes </strong><br>
    <strong>Swaminathan Gurumurthy</strong>, Tat-Jun Chin and Ian Reid <br>

    [7] 
[<a href='https://github.com/swami1995/vo-slam' target='_blank'>code</a>] <br>
    
<!-- <div id="abs_amos2020differentiable" style="text-align: justify; display: none" markdown="1"> -->
Built a package to construct a sparse map and camera trajectory using SIFT features, fine-tuned using bundle adjustment and loop closure. It was tailored for boreholes and underground scenes with forward motion, where most of the current state of the art approaches like LSD SLAM, ORB SLAM and SVO struggled at both localization and mapping.
<!-- </div> -->

</td>
</tr>

<tr>
<td class="col-md-3"><img src="images/publications/explore2018.png"/> </td>
<td>
    <strong> Off-on policy learning </strong><br>
    <strong>Swaminathan Gurumurthy</strong>, Bhairav Mehta, Anirudh Goyal <br>

    [8] 
    
<!-- <div id="abs_amos2020differentiable" style="text-align: justify; display: none" markdown="1"> -->
On policy methods are known to exhibit stable behavior and off-policy methods are known to be sample efficient. The goal here was to get the best of both worlds. We first developed a self-imitation based method to learn from a diverse set of exploratory policies which perform coordinated exploration. We also tried a meta-learning objective to ensure that the off-policy updates to the policies are aligned with future on-policy updates. This leads to more stable training but fails to reach peak performance in most continuous control tasks we tested on.
<!-- </div> -->

</td>
</tr>

<tr>
<td class="col-md-3"><img src="images/publications/interpretability2018.png"/> </td>
<td>
    <strong> Exploring interpretability in Atari Games for RL policies using Counterfactuals </strong><br>
    <strong>Swaminathan Gurumurthy</strong>, Akshat Agarwal, Prof. Katia Sycara <br>

    [9] 
    
<!-- <div id="abs_amos2020differentiable" style="text-align: justify; display: none" markdown="1"> -->
We aimed to understand what RL agents learn in simple games such as in Atari. We developed a GAN based method to find counterfactuals for the policies, i.e., we find small perturbations in the scene that can lead to changes in the agent action and use these to interpret agent behavior. GAN in this case is used to avoid adversarial examples and produce semantically meaningful perturbations.
<!-- </div> -->

</td>
</tr>


</table>
